---
author: "Hitesh Kumar"
date: "3 November 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Where is my age? - population dataset analysis

#### "I was hoping to do this in Python, but honestly it's much more
appropriate to do this in R." - me

Here we take a look at a relatively small dataset with information about the
population sizes across various locations throughout the UK and across various
ages.

In this short project, we will stick to the Question and Answer format given,
but might take some detours along the way.

### **I've summarised my answers in bullet points at the
start of each question**


```{r Libraries/Imports}
# Expect some function clashes, not important though
library(readxl)
library(ggplot2)
library(dplyr)
```

## Question 1

#### Summary:
* The collumn names weren't very freindly, so we gave them more meaningful
names
* Everything was stored as strings, not useful for dealing with numberslike age
or population size, we converted the collumn data types as appropriate
* Age is catagorical here, and one of the catagories, "90+" wasnt very freindly
so we replaced it by "90" and it means the same thing to us
* Geography code is most likely useless for us, and so isn't needed.
* I present some solutions to more minor problems at the end

Lets see what we are dealing with.

```{r Load and inspect}

raw_import_data = read_excel("population.xlsx", sheet = "Dataset")
head(raw_import_data, 10)

```

This dosent look very good. lets try to clean it up in place with some more
helpful collumn names and less clutter. We'll rename the collumns with the
orignal names (mistakenly put into the actual frame by the readxl function) and
remove the first two rows (empty row and names).

(We're only working with one dataset here, we know the context so lets just
call it "xxxxx_data")

```{r collumn naming}

# Always keep raw data away from any edits
cleaned_data =  raw_import_data

# Rename collumns with information from second row, remove useless rows
colnames(cleaned_data) = c(cleaned_data[2, ])
cleaned_data = cleaned_data[-1:-2, ]

head(cleaned_data, 10)
tail(cleaned_data, 10)

```

Additionally, we should make sure the data types of the values in the data
frame are appropriate. It seems that everything is stored as strings within
the frame. This also explains why 9 comes after 89 here! So lets convert age
and population sizes in each year to numbers.

One issue is that the Age catagory has an entry of "90+" which would result
in NULL values when forced to numeric (as.numeric() is very smart when
converting factors to numbers, but not that smart) . To counter this, we can
rename the 90+ catagory to just 90. This would preserve the information (as we
know now that the number 90 will mean all ages 90 and above) and make things
easy to work with. Converting age to a double as opposed to 8 bit int isnt too
important as performance isnt an issue yet.

```{r fixing data types 1}

# Replace all instances of '90' string with '90'
cleaned_data$Age[cleaned_data$Age == '90+'] = '90'
cleaned_data$Age = as.numeric(cleaned_data$Age)

head(cleaned_data, 10)
tail(cleaned_data, 10)

```

We should be aware that the boolean mask we are using to replace the values in
the data frame is a very ineffeicient method and is only really acceptable for
small datasets like this. For larger data sets, we would have to explore other
ways of finding values and assigning them to a set of data, such as converting
to a matrix, or even using external software.

Lets fix the other collumns too now:

```{r fixing data types 2}

# Convert all year collumns
cleaned_data[ , 5:8] = as.numeric(unlist(cleaned_data[ , 5:8]))

tail(cleaned_data, 10)

```

Ok, so finally it seems we have a nice looking dataset. We could still sort the
ages into ascending order by numbers, but it's not really an issues yet. If we
did have to however, we'd have to sort the data frame by age first and then by
geography to retain the alphabetical ordering of the geography collumn.

Now we still have quite a few tasks ahead of us. We should always check our
data before use (even before exploratory analysis) for any surpises or
inconsistencies etc. The checks we might want to do here are:

* Is there always a one to one mapping between geography and its code? **(Can check this by creating a hash map or dictionary and looking for any multiple values to any keys)**
* If not, does this mean that the Geography codes are neccessary or useless? **(Probabaly useless)**
* Does every geography have data for all age catagories? (integers from 0 to 90) **(Again, can create a hash map and compare values to each key against expectations)**
* Are there any missing entries **(Can create a heat map of the data frame by index and look for any 0's)**

But in this situation, its quite reasonable to assume that none of these
extreme cases will be realised here. Lets finally get rid of the geography code
collumn

```{r Removing geography code}

cleaned_data = cleaned_data[ , -2]

```

## Question 2

#### Summary:
* The smallest total population belonged to:
  * 2013:
  * 2014:
  * 2015:
  * 2016:

This should be quite a simple task, since we've converted the population values
to numbers, we find the total population size for each geography by summing
over each age catagory for each year. Lets demonstrate what we mean:

```{r filtering by sex}

# Only take the rows where the sex is "All"
total_by_geog = cleaned_data[cleaned_data$Sex == 'All', ]

head(total_by_geog, 10)

```

So now we have only information about all people in each age category
per geography, and since we know there are 91 catagories in age (0 to 90) we
can simply make a new frame which contains the sum of all ages per geography.

```{r total population per geography}

geography_set = unique(total_by_geog$Geography)

length(geography_set)
nrow(total_by_geog)

```

Hold on a second... We have 439 unique geographies in our dataset, across
40040 rows but we have (theoretically) 91 age catagories per geography. This
is contradictory as:

```{r Simple calculations}

40040/439

```

Implying that there are some geographies in the data set which have more than
91 age catagories!!?? Lets try to find how many age catagories each geography
has and single out a culprit.

```{r finding ages in geographies}

# Iterate through all geographies
for (i in 1:length(geography_set)) {
    
    # Find how many rows with that geography name occur
    suspect = geography_set[i]
    num_catagories = length(which(total_by_geog$Geography == suspect))
    
    # Single it out, report it
    culprits = c()
    if (num_catagories > 91) {
        cat(paste(suspect, "has", num_catagories, "catagories", sep = " "))
        culprits = c(culprits, suspect)
    }
    
}


```

So whats actually going on with West Midlands?

```{r Whats up with WM}

head(total_by_geog[total_by_geog$Geography == "West Midlands", ], 10)

```

So its repeated! Typical british naming conventions, no consistency, no pattern, 
no sense. We couldve kept (or reintroduce) the geography codes to help us split
these and rename them, but we dont need to as its clear that it alternates
between West Midlands 1 and West Midlands 2. 

Lets fix this:

```{r Fixing the West Midlands}

# Indexes for all "west midlands"" rows 
west_mid_indexes = c(which(total_by_geog$Geography == "West Midlands"))

# Assign them new names in an alternating pattern
for (i in west_mid_indexes) {
    
    if (i %% 2 == 1) {
        total_by_geog[i, 1] = "West Midlands 1"
    } else {
        total_by_geog[i, 1] = "West Midlands 2"
    }
    
}

total_by_geog[west_mid_indexes[1:10], ]


```

Finally, now we have a dataset we can use! Lets now do what we set out to do.
Lets find the toal per geography per year!

```{r total population per geography}

geography_set = unique(total_by_geog$Geography)

population_per_geog = data.frame(Geography = geography_set,
                                 total_2013 = c(rep(0, length(geography_set))),
                                 total_2014 = c(rep(0, length(geography_set))),
                                 total_2015 = c(rep(0, length(geography_set))),
                                 total_2016 = c(rep(0, length(geography_set)))
                                )

population_per_geog[1, 2] = sum(total_by_geog[1:91 , 4])
population_per_geog

```

```
for (i in length(geography_set)) {
    
    start_index = 91*(i-1) + 1
    end_index = 91*i
    
    population_per_geog[i, 2] = sum(total_by_geog[start_index:end_index , 4])
    population_per_geog[i, 3] = sum(total_by_geog[start_index:end_index , 5])
    population_per_geog[i, 4] = sum(total_by_geog[start_index:end_index , 6])
    population_per_geog[i, 5] = sum(total_by_geog[start_index:end_index , 7])
    
}

population_per_geog

```

```{r}
for (i in length(geography_set)) {
    
    start_index = 91*(i-1) + 1
    end_index = 91*i
    print(start_index)
    print(end_index)
    population_per_geog[geography_set[i], 2:5] = rowSums(total_by_geog[start_index:end_index , 4:7])
    
}

population_per_geog

```








